---
title: "Sampling Estimate, Precision, and Simulation"
output:
  xaringan::moon_reader:
    seal: false
    lib_dir: libs
    css: [default, shinobi, default-fonts, style.css]
    nature:
      beforeInit: "my_macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

class: middle, center
background-color: #a95c68

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(patchwork)
library(purrr)
opts_chunk$set(fig.height=7, 
               fig.width = 10,
               fig.align = "center",
               comment=NA, 
               warning=FALSE, 
               echo = FALSE,
               message = FALSE)

options(htmltools.dir.version = FALSE)
theme_set(theme_bw(base_size=18))
puce <- "#a95c68"
```

# Sampling Estimates, Precision, and Simulation

![](images/04/target.png)

### Biol 607

---

# Estimation and Precision

.large[
1. Probability Distributions and Population Parameter Estimates

2. Simulation, Precision, and Sample Size Estimation  

3. Bootstrapping our Way to Confidence

]
---
# Last Time: Sample Versus Population

```{r samp_pop_plot}
set.seed(2012)
samp <- rnorm(25, 45,5)
samp_data <- data.frame(size = samp)

sample_freq_plot <- ggplot(samp_data, aes(x = size, y = after_stat(density))) +
  geom_histogram(bins = 20) + 
  labs(y = "frequency")

samp_pop <- tibble(x = seq(45-15, 45+15, length.out = 200),
                   y = dnorm(x, 45, 5),
                   ymin = 0,
                   sd_size = "66%") %>%
  mutate(sd_size = case_when(
    x <= 45-10 ~ "5%",
    x >= 45+10 ~ "5%",
    x <= 45-5 ~ "95%",
    x >= 45+5 ~ "95%",
    TRUE ~ "66%"

  ))

sample_demo_plot <- sample_freq_plot +
  xlim(c(25,65)) +
  ggtitle("Sample")

pop_demo_plot <- ggplot()+ 
   geom_ribbon(data = samp_pop, 
              aes(x = x, ymax = y, ymin = ymin),
              fill = "red", alpha = 0.2) +  
   labs(title = "Population", x = "size")

sample_demo_plot + pop_demo_plot
```

---

# Sample Properties: **Mean**

$$\bar{x} = \frac{ \displaystyle \sum_{i=1}^{n}{x_{i}} }{n}$$

$\large \bar{x}$ - The average value of a sample  
$x_{i}$ - The value of a measurement for a single individual   
n - The number of individuals in a sample  
&nbsp;  
$\mu$ - The average value of a population  
(Greek = population, Latin = Sample)

---
class: center, middle

# Our goal is to get samples representative of a population, and estimate population parameters. We assume a **distribution** of values to the population.

---
# Probability Distributions
```{r normplot, fig.height=9, fig.width = 10}
x <- seq(-3,3,length.out=200)
normplot <- qplot(x, dnorm(x), geom = "line") +
  ylab("Probability Density") + xlab("")

normplot
```

---

# Probability Distributions Come in Many Shapes

```{r dists}
x1 <- seq(0,3,length.out=200)
x_discrete <- 0:30

expplot <- qplot(x1, dexp(x1), geom = "line") +
  ylab("Probability Density") + xlab("")

poisplot <- qplot(x_discrete, dpois(x_discrete, 10), geom = "col",
                 fill = I("white"), color = I("black")) +
  ylab("Probability Mass") + xlab("") 

binomsplot <- qplot(x_discrete, 
                    dbinom(x_discrete, size = 30, prob = 0.75), 
                    geom = "col", fill = I("white"), color = I("black")) +
  ylab("Probability Mass") + xlab("") 


normplot + 
  expplot +
  poisplot +
  binomsplot +
  plot_layout(ncol = 2)

```

---

# The Normal (Gaussian) Distribution

```{r normplot, fig.height=7, fig.width = 10, fig.align = "center"}
```

- Arises from some deterministic value and many small additive deviations
- VERY common
---
class: center

# Understanding Gaussian Distributions with a Galton Board (Quinqunx)

```{r quincunx, fig.show='animate', cache = TRUE}
#see also https://evamaerey.github.io/little_flipbooks_library/galton_board/galton_board.html#1
animation::quincunx(balls = 100)
```

---

# We see this pattern everywhere - the Random or Drunkard's Walk

```{r walk, echo = TRUE}
one_path <- function(steps){
  
  each_step <- c(0, runif(steps, min = -1, max = 1))

  path <- cumsum(each_step)
  
  return(path)
}
```

--

1. Input some number of steps to take 

2. Make a vector of 0 and a bunch of random numbers from -1 to 1  

3. Take the cummulative sum across the vector to represent the path  

4. Return the path vector


---

# 1000 Simulated Random Walks to Normal Homes
```{r all_walks}
steps <- 100
walk_sims <- map_df(1:1000, 
                    ~data.frame(sim = .x, 
                                time = -1*(0:steps), 
                                position = one_path(steps)))

ggplot(walk_sims,
       aes(x = time, y = position, group = sim)) +
  geom_line(alpha = 0.1) +
  coord_flip() +
  scale_x_continuous(breaks = -1*seq(0,steps, by = 10),
                     labels = seq(0,steps, by = 10))
```

---

# A Normal Result for Final Position

```{r final_walk}
walk_sims %>%
  filter(time == min(time)) %>%
  ggplot(aes(x = position)) +
  geom_histogram(fill = puce, bins = 200)
```

---

# Normal distributions
```{r normplot, fig.height = 5, fig.width = 5}
```

- Results from additive accumulation of many small errors
- Defined by a mean and standard deviation: $N(\mu, \sigma)$
- 2/3 of data is within 1 SD of the mean
- Values are peaked without **skew** (skewness = 0)
- Tails are neither too thin nor too fat (**kurtosis** = 0)
---

# Estimation and Precision

.large[
1. Probability Distributions and Population Parameter Estimates

2. .red[Simulation, Precision, and Sample Size Estimation]  

3. Bootstrapping our Way to Confidence

]

---
class: middle, center

# The Eternal Question: What should my sample size be?

---

# Let's find out
.large[
1. Get in groups of 3 <br><br>
2. Ask each other your age. Report the mean to me.<br><br>
3. Now get into another group of five, and do the same.<br><br>
4. Now get into another group of ten, and do the same.<br><br>
]
---

class: center, middle

# We simulated sampling from our class population!

---
# What if We Could Pretend to Sample?
.large[
- Assume the distribution of a population  

- Draw simulated 'samples' from the population at different sample sizes

- Examine when an estimated property levels off or precision is sufficient
      - Here we define Precision as 1/variance at a sample size
]
---
background-image: url(images/04/is-this-a-simulation.jpg)
background-position: center
background-size: cover

---

background-image: url(images/04/firefly-ship.jpg)
background-position: center
background-size: cover
class: center

# .inverse[Let's talk Firefly]

---
background-image: url(images/04/fireflies-1500x1000.jpg)
background-position: center
background-size: cover

```{r load_firefly}
firefly <- read.csv("lectures/data/04/04q06FireflyFlash.csv")
```
---
# Start With a Population...

Mean of Firefly flashing times: `r mean(firefly$flash.ms)`  
SD of Firefly flasing times: `r sd(firefly$flash.ms)`  

--

So assuming a normal distribution...

--

```{r fireflydist, fig.height = 5, fig.width = 7, echo=FALSE}
set.seed(607)
fireflyDens <- data.frame(x=seq(65,125, .1), 
                  y=dnorm(seq(65,125, .1), mean(firefly$flash.ms),sd(firefly$flash.ms)))

ffDensPlot <- ggplot(fireflyDens,aes(x=x, y=y)) +
         geom_line(lwd=1.5, color="blue") +
  theme_bw(base_size=17) + ylab("")

ffDensPlot
```

---
# Choose a Random Sample - n=5?

Mean of Firefly flashing times: `r mean(firefly$flash.ms)`  
SD of Firefly flasing times: `r sd(firefly$flash.ms)`  
So assuming a normal distribution...

```{r choose, echo=FALSE}
set.seed(607)
fireflySim5 <- rnorm(5,mean(firefly$flash.ms),sd(firefly$flash.ms))
```


```{r fireflydistPoints, fig.height = 5, fig.width = 7, echo=FALSE}
ffDensWithSim <- ffDensPlot +
  geom_point(data=data.frame(x=fireflySim5, y=0), mapping=aes(x=x, y=y), size=4)
ffDensWithSim
```


---

# Calculate Sample Mean

Mean of Firefly flashing times: `r mean(firefly$flash.ms)`  
SD of Firefly flasing times: `r sd(firefly$flash.ms)`  
So assuming a normal distribution...


```{r fireflydistMean,  fig.height = 5, fig.width = 7, echo=FALSE}
ffDensWithSim +
  geom_vline(xintercept=mean(fireflySim5), lwd=3, color="red", lty=2)
```

--
Rinse and repeat...

---
# How Good is our Sample Size for Estimating a Mean?
```{r distSim}
simFrame <- 
  data.frame(sampSize=rep(5:(nrow(firefly)+50), 10)) %>%
  mutate(sim=1:n()) %>%
  group_by(sim) %>%
  mutate(m = mean(rnorm(sampSize, mean(firefly$flash.ms), sd(firefly$flash.ms)))) %>%
  ungroup()

simFrame <- 
  data.frame(sampSize = 2:100) %>%
  group_by(sampSize) %>%
  summarize(m = map_dbl(1:1000, ~mean(rnorm(sampSize, 
                                      mean(firefly$flash.ms),
                                      sd(firefly$flash.ms)))))

simFrameSummary <- simFrame %>%
  group_by(sampSize) %>%
  summarize(mean_sim_sd = sd(m)) %>%
  ungroup() %>%
  arrange(mean_sim_sd)
```

```{r plot_dist_sim}
dist_sim <- ggplot(simFrame, mapping=aes(x=sampSize, y=m)) +
  theme_bw(base_size=17) +
  geom_point(alpha = 0.05) +
  xlab("Sample Size") +
  ylab("Estimate of Mean") 

dist_sim
```

---
# Where does the variability level off?
```{r dist_sim_stop}
dist_sim +
  geom_vline(xintercept=25, lwd=2, lty=2, color="red")
```

---
# Many Ways to Visualize

```{r bin2d}
ggplot(simFrame, mapping=aes(x=sampSize, y=m)) +
  theme_bw(base_size=17) +
  stat_bin_hex() +
  xlab("Sample Size") +
  ylab("Estimate of Mean") +
  scale_fill_viridis_c(option = "B")
```

---
# Note the Decline in SE with Increasing Sample Size

```{r se_and_n}
n_labelled <- function(string){
  return(paste0("n = ", string))
}

simFrame |>
  filter(sampSize %in% c(2,25,50,70)) |>
  ggplot(aes(x = m)) +
  geom_density() +
  facet_wrap(vars(sampSize), ncol = 2, labeller = as_labeller(n_labelled)) +
  labs(x = "estimated mean")
```


---
# Where does the variability level off?
```{r dist_sim_low_sd}
simFrameSummary %>% 
  arrange(sampSize) |>
  knitr::kable("html") %>%
  kableExtra::kable_minimal()
```

---

# Visualize Variability in Estimate of Mean

```{r sim_prec, fig.height = 6}
simPrec <- simFrame %>%
  group_by(sampSize) %>%
  summarize(precision = 1/var(m),
            sd = sd(m))

ggplot(simPrec,
       aes(x = sampSize, y = sd)) +
  xlab("Sample Size") +
  geom_line() + geom_point() +
  ylab("SD of Estimate of Mean\n(i.e., the SE)")
```


--
.large[What is acceptable to you? And/or relative to the Mean?]

---
class: center, middle
# **Central Limit Theorem** The distribution of means of a sufficiently large sample size will be approximately normal

https://istats.shinyapps.io/sampdist_cont/

https://istats.shinyapps.io/SampDist_discrete/


---
class: center, middle
# The Standard Error
<br><br>
.large[
A standard error is the standard deviation of an estimated parameter if we were able to sample it repeatedly. 
]

---

# But, I only Have One Sample? How can I know my SE for my mean other parameters?

```{r firehist, fig.height = 6}
ggplot(firefly,
       aes(x = flash.ms)) +
  geom_histogram(bins = 10, fill = puce, color = "black") +
  xlab("Flashes per ms")
```

---

# Estimation and Precision

.large[
1. Probability Distributions and Population Parameter Estimates

2. Simulation, Precision, and Sample Size Estimation  

3. .red[Bootstrapping our Way to Confidence]

]
---

# The Bootstrap
.large[

- We can resample our sample some number of times with replacement

- This resampling with replacement is called **bootstrapping**

- One replicate simulation is one **bootstrap**
]

---
# One Bootstrap Sample in R

```{r, echo = TRUE}
# One bootstrap
sample(firefly$flash.ms, 
       size = nrow(firefly), 
       replace = TRUE)
```

---

# Boostrapped Estimate of a SE
- We can calculate the Standard Deviation of a sample statistic from replicate bootstraped samples

- This is called the botstrapped **Standard Error** of the estimate

```{r, echo = TRUE}
one_boot <- function(){
  sample(firefly$flash.ms, 
         size = nrow(firefly), 
         replace = TRUE)
}

boot_means <- replicate(1000, mean(one_boot()))

sd(boot_means)
```

---
class: large

# So I always have to boostrap Standard Errors?

--

## .center[**No**]

--

Many common estimates have formulae, e.g.:  

$$SE_{mean} = \frac{s}{\sqrt(n)}$$

--

Boot SEM: Boot SEM: `r round(sd(boot_means),3)`, Est. SEM: `r round(sd(firefly$flash.ms)/sqrt(nrow(firefly)))`

--

.center[(but for medians, etc., yes )<br><br>https://istats.shinyapps.io/Boot1samp/]

---

# Bootstrapping to Estimate Precision of a Non-Standard Metric

```{r}
sd_boots <- replicate(1e3, sd(one_boot()))

qplot(sd_boots, geom = "histogram", bins = 30) +
  xlab("Standard Deviation of Flashes per ms") +
  ggtitle("1,000 Bootstrap Estimates of SD")
```

SE of the SD = `r round(sd(sd_boots), 3)`

---
class: large

# Standard Error as a Measure of Confidence
--

.center[.red[**Warning: this gets weird**]]

--


- We have calculated our SE from a **sample** - not the **population**

--

- Our estimate Â± 1 SE tells us 2/3 of the *means* we could get by **resampling this sample**  

--

- This is **not** 2/3 of the possible **true parameter values**  

--

- BUT, if we *were* to sample the population many times, 2/3 of the time, the sample-based SE will contain the "true" value


---

# Confidence Intervals

.large[
- So, 1 SE = the 66% Confidence Interval  

- ~2 SE = 95% Confidence Interval  

- Best to view these as a measure of precision of your estimate

- And remember, if you were able to do the sampling again and again and again, some fraction of your intervals would *not* contain a true value
]
---

class: large, middle

# Let's see this in action

### .center[.middle[https://istats.shinyapps.io/ExploreCoverage/]]

---

# Frequentist Philosophy

.large[The ideal of drawing conclusions from data based on properties derived from theoretical resampling is fundamentally **frequentist** - i.e., assumes that we can derive truth by observing a result with some frequency in the long run.]

```{r CI_sim, fig.height = 5}
one_sim <- function(){
  p <- rnorm(25, 4, 5)
  tibble(m = mean(p),
         s = sd(p),
         se = s/sqrt(25),
         lwr = m - se,
         upr = m + se)
}

dat <- purrr::map_df(1:100, ~one_sim()) %>%
  mutate(sim = 1:100,
         cover = ifelse(lwr < 4 & upr > 4, "yes", "no"))

ggplot(dat,
       aes(x = sim, y = m,
           ymin = lwr, ymax = upr,
           color = cover)) +
  geom_pointrange() +
  geom_hline(yintercept = 4, lty = 2) +
  coord_flip() +
  xlab("") + ylab("mean") +
  labs(subtitle = paste(sum(dat$cover=="yes"), "% Overlap True Value"), color = "Covers\nTrue\nValue?") +
  scale_color_manual(values = c(puce, "darkblue"))

```

---
# To Address some Confusion: SE (sample), CI (sample), and SD (population)....

 ![:scale 55%](images/04/cumming_comparison_2007.jpg) 


.bottom[.small[.left[[Cumming et al. 2007 Table 1](http://byrneslab.net/classes/biol-607/readings/Cumming_2007_error.pdf)]]]


---
# SE, SD, CIs....
![:scale 75%](images/04/cumming_table_2007.jpg)

.bottom[.small[.left[[Cumming et al. 2007 Table 1](http://byrneslab.net/classes/biol-607/readings/Cumming_2007_error.pdf)]]]