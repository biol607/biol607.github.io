---
title: "Sampling and Simulation"
output:
  xaringan::moon_reader:
    seal: false
    lib_dir: libs
    css: [default, shinobi, default-fonts, style.css]
    nature:
      beforeInit: "my_macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: center, middle

# Evaluating Fit Linear Models
<br>
![:scale 55%](images/12/linear_regression_love.gif)

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(mvtnorm)
library(broom)
library(performance)

opts_chunk$set(fig.height=6, 
               fig.width = 8,
               fig.align = "center",
               comment=NA, 
               warning=FALSE, 
               echo = FALSE,
               message = FALSE)

options(htmltools.dir.version = FALSE)
theme_set(theme_bw(base_size=18))
```


```{r puffer, include=FALSE}
puffer <- read.csv("lectures/data/11/16q11PufferfishMimicry Caley & Schluter 2003.csv")
puffer_lm <- lm(predators ~ resemblance, data=puffer)
```


---


# Putting Linear Regression Into Practice with Pufferfish

```{r pufferload}
puffer <- read.csv("lectures/data/11/16q11PufferfishMimicry Caley & Schluter 2003.csv")
```

.pull-left[
- Pufferfish are toxic/harmful to predators  
<br>
- Batesian mimics gain protection from predation - why?
<br><br>
- Evolved response to appearance?
<br><br>
- Researchers tested with mimics varying in toxic pufferfish resemblance
]

.pull-right[
![:scale 80%](./images/11/puffer_mimics.jpg)
]
---
## Question of the day: Does Resembling a Pufferfish Reduce Predator Visits?
```{r puffershow}
pufferplot <- ggplot(puffer, mapping=aes(x=resemblance, y=predators)) +
  ylab("Predator Approaches per Trial") + 
  xlab("Dissimilarity to Toxic Pufferfish")  +
  geom_point(size = 3) +
  theme_bw(base_size=24) 

pufferplot + stat_smooth(method = "lm")
```

```{r wolf_scatterplot, include = FALSE}
wolves <- read.csv("lectures/data/11/16e2InbreedingWolves.csv") %>%
  mutate(inbreeding_coefficient = inbreeding.coefficient)

wolfplot <- ggplot(data=wolves, mapping=aes(x=inbreeding.coefficient, y=pups)) +
xlab("Inbreeding Coefficient") + ylab("# of Pups") +
geom_point(size=3) +
theme_bw(base_size=24) 

wolf_mod <- lm(pups ~ inbreeding_coefficient,
               data = wolves)

wolfplot + stat_smooth(method = "lm")
```


---

# Digging Deeper into Regression

1. Assumptions: Is our fit valid? 

2. How did we fit this model?


---

# You are now a Statistical Wizard. Be Careful. Your Model is a Golem.
(sensu Richard McElreath)


.center[.middle[![:scale 45%](images/09/golem.png)]]

---

# A Case of "Great" versus "Not as Great" Fits...

.pull-left[

![:scale 80%](./images/11/puffer_mimics.jpg)

]

.pull-right[

![](./images/11/CUTE_WOLF_PUPS_by_horsesrock44.jpeg)

]

---
# The Two Fits

.pull-left[
<br><br>
```{r puffershow, fig.height=5, fig.width=5}
```

]

.pull-right[

<br><br>
```{r wolf_scatterplot, fig.height=5, fig.width=5}
```
]

---
# Assumptions (in rough descending order of importance)

1. Validity

2. Representativeness

3. Model captures features in the data

4. Additivity and Linearity

5. Independence of Errors

6. Equal Variance of Errors 

7. Normality of Errors

8. Minimal Outlier Influence

---
# Validity: Do X and Y Reflect Concepts I'm interested In


```{r puffershow, fig.height=7, fig.width=7}
```


What if predator approaches is not a good measure of recognition?  Or mimics just don't look like fish?

---
class: middle

# Solution to lack of validity:  


## Reframe your question! Change your framing! Question your life choices!

---

# Representativeness: Does Your Data Represent the Population?

#### For example, say this is your result...


```{r predictionRange1}
set.seed(10201)
par(mfrow=c(1,2))
x <- sort(runif(100,0,100))
y1 <- rnorm(10, 0.5*x[51:60], 2)
y <- rnorm(100, 0.5*x, 2)

poorlm <- lm(y1 ~ x[51:60])
plot(y1 ~ x[51:60], pch=19, xlab="X", ylab="Y")
aplot <- qplot(x[51:60], y1)  + theme_bw(base_size=16) + xlab("X") + ylab("Y")+ stat_smooth(method="lm", lwd=1.5, col="red", lty=2)
aplot
```

---
class: center, middle

# But is that all there is to X in nature?

---
# Representativeness: Does Your Data Represent the Population?

#### What if you are looking at only a piece of the variation in X in your population?

```{r predRange2}
aplot+xlim(c(0,100))
```

---
# Representativeness: Does Your Data Represent the Population?

#### How should you have sampled this population for a representative result?

```{r PredictionRange3}
bplot <- aplot+geom_point(mapping=aes(x=x, y=y))  + 
  theme_bw(base_size=16) + xlab("X") + ylab("Y")
bplot
```

---
# Representativeness: Does Your Data Represent the Population?

#### It's better to have more variation in X than just a bigger N


```{r predRange4}
y2 <- y[seq(0,100,10)]
x2 <- x[seq(0,100,10)]
cplot <- qplot(x2, y2) + geom_point(size=3)  + theme_bw(base_size=16) + xlab("X") + ylab("Y")+ stat_smooth(method="lm", lwd=1.5, col="blue", lty=1)
cplot
```

---
# Representativeness: Does Your Data Represent the Population?

- Always question if you did a good job sampling

- Use natural history and the literature to get the bounds of values

- If experimenting, make sure your treatment levels are representative

- If you realize post-hoc they are not, **qualify your conclusions**

---
# Model captures features in the data
```{r puffershow}
```

Does the model seem to fit the data? Are there any deviations? Can be hard to see...


---
# Simulating implications from the model to see if we match features in the data

```{r sims}
# sims_simulate <- simulate(puffer_lm, nsim = 100)%>%
#   pivot_longer(cols = everything(),
#                names_to = "sim",
#                values_to = "predators")
# 
# 
# ggplot() +
#   geom_density(data = sims_simulate, 
#                aes(x = predators, group = sim), lwd = 0.1) +
#   geom_density(data = puffer, aes(x = predators),
#                lwd = 2, color = "blue") +
#   labs(title = "Distribution of Predator Visits in Our Data\nand as Predicted By Model")
# 
check_predictions(puffer_lm)
```

Is anything off?

---
# But what to wolves say to you?

```{r wolfsims}
# sims_simulate <- simulate(wolf_mod, nsim = 100)%>%
#   pivot_longer(cols = everything(),
#                names_to = "sim",
#                values_to = "pups")
# 
# 
# ggplot() +
#   geom_density(data = sims_simulate, 
#                aes(x = pups, group = sim), lwd = 0.1) +
#   geom_density(data = wolves, aes(x = pups),
#                lwd = 2, color = "blue") +
#   labs(title = "Distribution of Pups in Our Data\nand as Predicted By Model")

check_predictions(wolf_mod)
```

---

# Additivity and Linearity: Should account for all of the variation between residual and fitted values - what you want

```{r pufferadd}

ggplot(data = puffer,
       aes(x = predators, y = fitted(puffer_lm))) +
  geom_point() +
  stat_smooth(method = "lm", fill = NA)
```

---

# Additivity and Linearity: Wolf Problems?

```{r wolfadd}
ggplot(data = wolves,
       aes(x = pups, y = fitted(wolf_mod))) +
  geom_point() +
  stat_smooth(method = "lm", fill = NA)
```

--
**Solutions:** Nonlinear transformations or a better model!

---

# Independence of Errors

- Are all replicates TRULY independent

- Did they come from the same space, time, etc.

- Non-independence can introduce **BIAS**
     - SEs too small (at the least)
     - Causal inference invalid
     
- Incoporate Non-independence into models (many methods)

---

# Equal Variance of Errors: No Pattern to Residuals and Fitted Values

```{r resfit_puffer}
library(ggfortify)
autoplot(puffer_lm, which = 1, ncol = 1)
#check_model(wolf_mod, "linearity")

```

---

# Equal Variance of Errors: What is up with intermediate Wolf Values
```{r resfit}
autoplot(wolf_mod, which = 1, ncol = 1)
#check_model(wolf_mod, "linearity")
```
---

# Equal Variance of Errors: Problems and Solutions

- Shapes (cones, footballs, etc.) with no bias in fitted v. residual relationship

- A linear relationship indicates an additivity problem

- Can solve with a better model (more predictors)

- Can solve with weighting by X values, if source of heteroskedasticity known
     - This actually means we model the variance as a function of X
     - $\epsilon_i \sim(N, f(x_i))$
 
- Minor problem for coefficient estimates

- Major problem for doing inference and prediction as it changes error

---
# Normality of errors: Did we fit the error generating process that we observed?

- We assumed $\epsilon_i \sim N(0,\sigma)$ - but is that right?

- Can assess with a QQ-plot
     - Do quantiles of the residuals match quantiles of a normal distribution?
     
- Again, minor problem for coefficient estimates  

- Major problem for doing inference and prediction, as it changes error

---
# Equal Variance of Errors: Puffers

```{r pufferqq}
#autoplot(puffer_lm, which = 2, ncol = 1)
check_normality(puffer_lm) |> plot(type = "qq")

```

---
# Equal Variance of Errors: Wolves underpredict at High Levels

```{r wolfqq, fig.height = 5, fig.width = 6}
#autoplot(wolf_mod, which = 2, ncol = 1)
check_normality(wolf_mod) |> plot(type = "qq")
```


---
# Outliers: Cook's D

```{r pufferout}
#autoplot(puffer_lm, ncol = 1, which = 4)
check_outliers(puffer_lm, method = "cook") |> plot(type = "bar")

```

---
# Leverage: Cook's D Scaled by Value

```{r pufferout_leverage}
#autoplot(puffer_lm, ncol = 1, which = 4)
check_outliers(puffer_lm, method = "cook") |> plot()

```

---
# Leverage: Cook's D - wolves OK

```{r pufferout_cook}
#autoplot(wolf_mod, ncol = 1, which = 4)
check_outliers(wolf_mod, method = "cook") |> plot()
```

---

# Everyone worries about outliers, but...

- Are they real?

- Do they indicate a problem or a nonlinearity?

- Remove only as a dead last resort

- If from a nonlinearity, consider transformation

---

# Assumptions (in rough descending order of importance)

1. Validity: only you know!

2. Representativeness: look at nature

3. Model captures features in the data: compare model v. data!

4. Additivity and Linearity: compare model v. data!

5. Independence of Errors: consider sampling design

6. Equal Variance of Errors: evaluate res-fit 

7. Normality of Errors: evaluate qq and levene test

8. Minimal Outlier Influence: evaluate Cook's D

---

# Digging Deeper into Regression

1. Assumptions: Is our fit valid? 

2. .red[How did we fit this model?]


---
# So, uh.... How would you fit a line here?

```{r puffer_only_scatter}
pufferplot
```

---

# Lots of Possible Lines - How would you decide?

```{r lsq}
library(mnormt)
set.seed(697)
x<-1:10
y<-rnorm(10, mean=x,sd=2)
a<-lm(y~x)
ab <- rmnorm(3, coef(a), vcov(a))

par(mfrow=c(1,3))
for(i in 1:3){
  plot(x,y,pch=19, cex=1.5)
  abline(a=ab[i,1], b=ab[i,2], lwd=2)
  segments(x,ab[i,1] + x*ab[i,2],x,y, col="red", lwd=2)
}

```

---

# Method of Model Fitting

1. Least Squares
  - Conceptually Simple
  - Minimizes distance between fit and residuals
  - Approximations of quantities based on frequentist logic
  
2. Likelihood
  - Flexible to many models
  - Produces likelihood surface of different parameters 
  - Equivalent to LS for Gaussian likelihood
  - Approximations of quantities based on frequentist logic

3. Bayesian
  - Incorporates prior knowledge
  - Probability for any parameter is likelihood * prior
  - Superior for quantifying uncertainty
  - With "flat" priors, equivalent to least squares/likelihood
  - Analytic or simulated calculation of quantities

---

# Basic Principles of Least Squares Regression

$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ where $\beta_0$ = intercept, $\beta_1$ = slope. 

$\epsilon_i \sim \mathcal{N}(0, \sigma)$ - the residuals

```{r linefit}
set.seed(697)
x<-1:10
y<-rnorm(10, mean=x,sd=2)
a<-lm(y~x)
plot(x,y,pch=19, cex=1.5)
abline(a, lwd=2)
segments(x,fitted(a),x,y, col="red", lwd=2)
dat <- tibble(x=x, y = y,
              mean_resid = y - mean(y),
              fit = fitted(lm(y~x)),
              resid = residuals(lm(y~x)))
``` 
---

# Basic Principles of Least Squares Regression: Total Sums of Squares

$$SST = \sum (Y_i - \bar{Y})^2$$
```{r sst}

ggplot(dat, aes(x=x, y=y)) +
  geom_point() +
  geom_hline(yintercept = mean(dat$y), color = "purple") +
  labs(
    subtitle = "purple = mean of y") +
  ylim(c(0,12))
```

---
# Basic Principles of Least Squares Regression: Total Sums of Squares

$$SST = \sum (Y_i - \bar{Y})^2$$

```{r sst2}

ggplot(dat, aes(x=x, y=y)) +
  geom_point() +
  geom_hline(yintercept = mean(dat$y), color = "purple") +
  geom_segment(aes(y = mean(y),
                   yend =  mean(y) + mean_resid, 
                   xend = x),
               color = "blue")+
  labs(
    subtitle = "purple = mean of y\nblue = distance to observed y")+
  ylim(c(0,12))
```
---
class: center, middle, large
# Sums of Squares of a Model

SST = SS Model + SS Residuals

--

We want to minimize SS Residuals

---

# Basic Principles of Least Squares Regression: Sums of Squares of the Model

$$SSM = \sum (\widehat{Y_i} - \bar{Y})^2$$


```{r ssm}

ggplot(dat, aes(x=x, y=y)) +
  geom_hline(yintercept = mean(dat$y), color = "purple") +
  geom_line(aes(y = fit)) +
  geom_segment(aes(y = mean(y),
                   yend =  fit, 
                   xend = x),
               color = "orange", lwd = 1.5)+
    geom_point() +
  labs(
    subtitle = "purple = mean of y\norange = distance to fit y")+
  ylim(c(0,12))
```

---
# Basic Principles of Least Squares Regression: Sums of Squares of the Residuals
Minimize Residuals defined as $SS_{residuals} = \sum(Y_{i} - \widehat{Y})^2$

```{r ssr}

ggplot(dat, aes(x=x, y=y)) +
  geom_hline(yintercept = mean(dat$y), color = "purple") +
  geom_line(aes(y = fit)) +
  geom_segment(aes(y = mean(y),
                   yend =  fit, 
                   xend = x),
               color = "orange", lwd = 1.5)+
  geom_segment(aes(y = fit,
                   yend =  y, 
                   xend = x),
               color = "red", lwd = 1.5)+
    geom_point() +
  labs(
    subtitle = "purple = mean of y\norange = distance to fit y\nred = residual distance")+
  ylim(c(0,12))
```

---
class: center, middle

# Is there Another Way?

---

# Analytic Solution: Solving for Slope
<br><br>

$\LARGE b=\frac{s_{xy}}{s_{x}^2}$ $= \frac{cov(x,y)}{var(x)}$

--

$\LARGE = r_{xy}\frac{s_{y}}{s_{x}}$



---

# Analytic Solution: Solving for Intercept
<br><br>
Least squares regression line always goes through the mean of X and Y  


$\Large \bar{Y} = \beta_0 + \beta_1 \bar{X}$

<br><br>

--
$\Large \beta_0 = \bar{Y} - \beta_1  \bar{X}$


---

# Least Squares Visualized for Puffers

```{r}
library(modelr)
puffer_m <- puffer |> add_residuals(puffer_lm) |> add_predictions(puffer_lm)

ggplot(puffer_m,
       aes(x = resemblance)) +
  geom_point(aes(y = predators)) +
  stat_smooth(method = "lm", mapping = aes(y = predators)) +
  geom_point(aes(y = pred), color = "blue", size = 2) +
  geom_segment(aes(xend = resemblance, yend = predators,
                   y = pred), color = "red")
```


---
# Likelihood
  - Flexible to many models
  - Produces likelihood surface of different parameters 
  - Equivalent to LS for Gaussian likelihood
  - Approximations of quantities based on frequentist logic


--
$L = \prod p(Data|parmeters)$
--
$L(\theta | D) = \prod dnorm(y_i, \mu = \beta_0 + \beta_1 x_i, \sigma)$  

--

Deviance = -2 * Log Likelihood  

--
This gives us the same answer as Least Squares

---

# Likelihood: Minimizing Deviance (Maximizing Likelihood) by Search

```{r,  message=FALSE, warning=FALSE}
library(profileModel)
puffer_glm <- glm(formula = predators ~ resemblance, data = puffer)
profileModel(puffer_glm,
                     objective = "ordinaryDeviance",
             quantile = qchisq(0.66, 1)) |> plot()
```

---

# Bayesian
  - Incorporates prior knowledge
  - Probability for any parameter is likelihood * prior
  - Superior for quantifying uncertainty
  - With "flat" priors, equivalent to least squares/likelihood
  - Analytic or simulated calculation of quantities


$$p(H|D) = \frac{p(D|H)p(H)}{p(D)}$$

---

# Bayes: Creating a Posterior Probability Distribution

```{r, cache = TRUE}
library(rstanarm)

p_chain <- stan_glm(predators ~ resemblance, data = puffer)

plot(p_chain, "trace")
```

--

Searches $p(H|D) = \frac{p(D|H)p(H)}{p(D)}$

---

# Bayes: Creating a Posterior Probability Distribution
```{r}
plot(p_chain, "areas")

```

---

# Linear Regression - the Core of Everything

- Make sure you meet assumptions  
      - Don't burn down Prague


- Many ways to fit   
      - We will talk inference later
      - The key is looking at estimated values and their implications
      - Look at precision - do you feel comfortable with inference?