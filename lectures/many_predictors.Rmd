---
title:
output:
  revealjs::revealjs_presentation:
    reveal_options:
      slideNumber: true
      previewLinks: true
    theme: white
    center: false
    transition: fade
    self_contained: false
    lib_dir: libs
    css: style.css
---

## 
```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(fig.height=4.5, comment=NA, 
               warning=FALSE, message=FALSE, 
               dev="jpeg", echo=FALSE)
library(dplyr)
library(tidyr)
library(broom)
library(ggplot2)
library(car)
library(visreg)
library(emmeans)
library(performance)

theme_set(theme_bw(base_size = 16))

```
![](images/23/wonka_mult_regression.jpg)
<h2> Multiple Predictor Variables in Linear Models</h2>


<!-- Next time, 

show log transform and predictions at the end.
add part on supression of effects/simpsons's paradox lite
Port to Xaringan

-->

##
\ 
\
\
<h3>https://etherpad.wikimedia.org/p/607-many-predictors-2022</h3>

## Data Generating Processes Until Now

-   One predictor with one response\
    \

-   Or multiple possible treatment levels, each 0/1

## We Have Seen Many Predictors Before

$$y_{ij} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{ij}, \qquad x_{i} = 0,1$$  

```{r load_brains}
brainGene <- read.csv("lectures/data/19/15q06DisordersAndGeneExpression.csv") %>%
  mutate(group = forcats::fct_relevel(group, c("control", "schizo", "bipolar")))

ggplot(brainGene, aes(x=group, y=PLP1.expression, fill = group)) +
  geom_boxplot() +
  scale_fill_discrete(guide=FALSE)+
  theme_bw(base_size=17)
```


## What if We Had Multiple Continuous Predictors?


$$y_{i} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{i}$$
\
$$\epsilon_{i} \sim \mathcal{N}(0, \sigma)$$
- Here, $x_{ij}$ is the *continuous* level of predictor j in replicate i  
\
- And $\beta_{j}$ is now the slope for predictor j

## The Expansiveness of the Linear Model

$$\Large \boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon} $$  

-   This equation is huge. X can be anything - categorical,
    continuous, squared, sine, etc.

-   There can be straight additivity, or interactions



## Many Continuous Predictors: Multiple Linear Regression

1.   Multiple Linear Regression  
\
2.   Assumptions and Multicollinearity  
\
3. Inferences with MLR

## Multiple Linear Regression: A Graphical View

![image](./images/23/regression2.png){width="60.00000%"}


<div style="text-align:left">Curved double-headed arrow indicates COVARIANCE between predictors that we account for.  
\
We estimate the effect of each predictor **controlling** for all others.  
\
</div>


## Calculating Multiple Regression Coefficients with OLS

$$\boldsymbol{Y} = \boldsymbol{b X} + \boldsymbol{\epsilon}$$

<div style="text-align:left">
Remember in Simple Linear Regression $b = \frac{cov_{xy}}{var_{x}}$?\
\
In Multiple Linear Regression
$\boldsymbol{b} = \boldsymbol{cov_{xy}}\boldsymbol{S_{x}^{-1}}$\
\
where $\boldsymbol{cov_{xy}}$ is the covariances of $\boldsymbol{x_i}$
with $\boldsymbol{y}$ and $\boldsymbol{S_{x}^{-1}}$ is the
variance/covariance matrix of all *Independent variables*\
</div>


##  {data-background="images/23/fires.jpg"}
<div style="bottom:100%; text-align:left; background:goldenrod">Five year study of wildfires & recovery in Southern California shurblands in 1993. 90 plots (20 x 50m)  
\
(data from Jon Keeley et al.)</div>



## What causes species richness?

- Distance from fire patch 
- Elevation
- Abiotic index
- Patch age
- Patch heterogeneity
- Severity of last fire
- Plant cover

## Many Things may Influence Species Richness

```{r keeley_pairs}
keeley <- read.csv("lectures/data/23/Keeley_rawdata_select4.csv")
pairs(keeley)
```

## Our Model
\
\
$$Richness_i =\beta_{0}+ \beta_{1} cover_i +\beta_{2} firesev_i + \beta_{3}hetero_i +\epsilon_i$$
\
\
<div class="fragment">

```{r mlr, echo=TRUE}

klm <- lm(rich ~ cover + firesev + hetero, data=keeley)
```
</div>

## Many Continuous Predictors: Multiple Linear Regression

1.   Multiple Linear Regression  
\
2.   <font color = "red">Assumptions and Multicollinearity</font>  
\
3. Inferences with MLR

## Testing Assumptions
> - Data Generating Process: Linearity \
\
> - Error Generating Process: Normality & homoscedasticity of residuals  
\
> - Data: Outliers not influencing residuals  
\
> - Predictors: **Minimal multicollinearity**

## Did We Match our Data?

```{r}
check_predictions(klm)
```

## How About That Linearity?

```{r}
check_model(klm, check = "linearity") |> plot()
```

## OK, Normality of Residuals?
```{r}
check_normality(klm) |> plot()
```

## OK, Normality of qResiduals?
```{r}
check_normality(klm) |> plot("qq")
```

## No Heteroskedasticity?
```{r}
check_heteroscedasticity(klm) |> plot()
```

## Outliers?
```{r}
check_outliers(klm) |> plot()
```

## 
![](./images/23/gosling_multicollinearity.jpg)

## Why Worry about Multicollinearity?

> - Adding more predictors decreases precision of estimates  
\
> - If predictors are too collinear, can lead to difficulty fitting model  
\
> - If predictors are too collinear, can inflate SE of estimates further  
\
> - If predictors are too collinear, are we *really* getting **unique information**

## Checking for Multicollinearity: Correlation Matrices

```{r klm_cor, size="normalsize"}
with(keeley, cor(cbind(cover, firesev, hetero)))
``` 

<div style="text-align:left">Correlations over 0.4 can
be problematic, but, meh, they may be OK even as high as 0.8. \
</div>


## Checking for Multicollinearity: Variance Inflation Factor

> - Consider $y = \beta_{0} + \beta_{1}x_{1}  + \beta_{2}x_{2} + \epsilon$ \
\
> - And $X_{1} = \alpha_{0} + \alpha_{2}x_{2} + \epsilon_j$ \
\
> - $var(\beta_{1}) = \frac{\sigma^2}{(n-1)\sigma^2_{X_1}}\frac{1}{1-R^{2}_1}$
\
\
<span class="fragment">$$VIF = \frac{1}{1-R^2_{1}}$$ </span>

## Checking for Multicollinearity: Variance Inflation Factor
$$VIF_1 = \frac{1}{1-R^2_{1}}$$ 


```{r klm_vif, fig.height = 4}
check_collinearity(klm) |> plot()
``` 

<div style="text-align:left">VIF $>$ 5 or 10 can be problematic and indicate an unstable solution.</div>

## What Do We Do with High Collinearity?

> - Cry.  
\
> - Evaluate **why**  
\ 
> - Can drop a predictor if information is redundant  
\
> - Can combine predictors into an index
>     - Add them? Or other combination.  
>     - PCA for orthogonal axes  
>     - Factor analysis to compress into one variable

## Many Continuous Predictors: Multiple Linear Regression

1.   Multiple Linear Regression  
\
2.   Assumptions and Multicollinearity  
\
3. <font color = "red">Inferences with MLR</font>

## What does it all mean: the coefficients
$$Richness_i =\beta_{0}+ \beta_{1} cover_i +\beta_{2} firesev_i + \beta_{3}hetero_i +\epsilon_i$$
```{r keeley_coe}
tidy(klm) |>
  dplyr::select(1:3) |>
  knitr::kable(digits = 2) |>
  kableExtra::kable_styling()
``` 

> - $\beta_0$ - the intercept -  is the # of species when **all other predictors are 0**  
>     - Note the very large SE  
  
>  - All other $\beta$s are the effect of a 1 unit increase on # of species  
>     - They are **not** on the same scale
>     - They are each in the scale of species per unit of individual x

## How Much Variation is Associated with the Predictors
```{r}
glance(klm) |>
  dplyr::select(1:2)  |>
  knitr::kable(digits = 2) |>
  kableExtra::kable_styling()
  
```

- 41% of the varition in # of species is associated with the predictors  
\
- Note that this is **all model**, not individual predictors 

## Comparing Coefficients on the Same Scale

$$r_{xy} = b_{xy}\frac{sd_{x}}{sd_{y}}$$ 

```{r keeley_std}
library(effectsize) 
effectsize(klm, method = "basic")
```

- For linear model, makes intuitive sense to compare strength  
\
- Note, this is Pearson's correlation, so, it's in units of $sd_y/sd_x$

## So, Uh, How Do We Visualize This?

```{r klm_see_effects}

qplot(cover, rich, data=keeley, colour=firesev, size=firesev) +
  theme_bw(base_size=14) + 
  scale_color_gradient(low="yellow", high="purple") +
  scale_size_continuous(range=c(1,10))
```

## Visualization Strategies for Multivariate Models

- Plot the effect of each variable holding the other variables constant  
     - Mean, Median, 0
     - Or your choice!  
\
- Plot **counterfactual scenarios** from model
     - Can match data (and be shown as such)
     - Can be exploring the response surface



## Added Variable Plot to Show Unique Contributions when Holding Others at 0
```{r klm_avplot}
avPlots(klm)
```

## Plots at Median of Other Variables

```{r klm_visreg, fig.height=6, fig.width = 10}
library(patchwork)
klm_vr <- visreg::visreg(klm, cex.lab=1.3,  gg = TRUE)

klm_vr[[1]] + 
klm_vr[[2]] +
klm_vr[[3]]
``` 

## Counterfactual Predictions Overlaid on Data

```{r crazy}
pred_info <- crossing(cover = seq(0,1.5, length.out=100),
                      firesev=c(2,5,8)) %>%
  crossing(hetero=c(0.5, 0.8)) %>%
  modelr::add_predictions(klm, var="rich") %>%
  dplyr::mutate(hetero_split = hetero)

het_labeller <- as_labeller(c(`0.5` = "hetero: 0.5", `0.8` = "hetero: 0.8"))


ggplot(pred_info, mapping=aes(x=cover, y=rich)) +
  geom_line(lwd=1.5, mapping=aes(color=factor(firesev), group=paste(firesev, hetero))) +
    facet_wrap(vars(hetero_split), labeller = het_labeller) +
  geom_point(data=keeley %>% 
               dplyr::mutate(hetero_split = ifelse(hetero<mean(hetero), 0.5, 0.8))) +
  theme_bw(base_size=14) +
  labs(color = "firesev")
```

## Counterfactual Surfaces at Means of Other Variables
```{r}
visreg::visreg2d(klm, "cover", "firesev")
```


## 
\
\
![](images/23/matrix_regression.jpg)

This is an incredibly powerful technique at teasing apart different correlated influences!

